{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import cv2\n",
    "from os import listdir\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tf.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_and_normalize(arr):\n",
    "    \"\"\"\n",
    "    Perform Positive Global Standardization on input array and return it.\n",
    "    Arguments:\n",
    "        arr: 2-dimensional image array containing int or float values\n",
    "    Returns:\n",
    "        arr: positive globally standardized arr of float values\n",
    "    \"\"\"\n",
    "    arr = arr.astype('float32')\n",
    "    mean, stand_dev = arr.mean(), arr.std()\n",
    "    arr = (arr-mean)/stand_dev\n",
    "    arr = np.clip(arr, -1, 1)\n",
    "    arr = (arr+1)/2\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dir_list, image_size):\n",
    "    \"\"\"\n",
    "    Read images, resize and normalize them. \n",
    "    Arguments:\n",
    "        dir_list: list of strings representing file directories.\n",
    "    Returns:\n",
    "        X: A numpy array with shape = (#_examples, image_width, image_height, #_channels)\n",
    "        y: A numpy array with shape = (#_examples, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # load all images in a directory\n",
    "    X = []\n",
    "    y = []\n",
    "    image_width, image_height = image_size\n",
    "    \n",
    "    for directory in dir_list:\n",
    "        for filename in listdir(directory):\n",
    "            # load the image\n",
    "            image = cv2.imread(directory + '/' + filename)\n",
    "            # crop the brain and ignore the unnecessary rest part of the image\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            # resize image\n",
    "            image = cv2.resize(image, dsize=(image_width, image_height), interpolation=cv2.INTER_CUBIC)\n",
    "            # normalize values\n",
    "            image = scale_and_normalize(image)\n",
    "            # convert image to numpy array and append it to X\n",
    "            X.append(image)\n",
    "            # append a value of 1 to the target array if the image\n",
    "            # is in the folder named 'yes', otherwise append 0.\n",
    "            if directory[-3:] == 'yes':\n",
    "                y.append([1])\n",
    "            else:\n",
    "                y.append([0])\n",
    "                \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Shuffle the data\n",
    "    X, y = shuffle(X, y)\n",
    "    \n",
    "    print(f'Number of examples is: {len(X)}')\n",
    "    print(f'X shape is: {X.shape}')\n",
    "    print(f'y shape is: {y.shape}')\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples is: 253\n",
      "X shape is: (253, 240, 240)\n",
      "y shape is: (253, 1)\n"
     ]
    }
   ],
   "source": [
    "# Specify standard dimensions, load data from parent folder\n",
    "\n",
    "IMG_WIDTH, IMG_HEIGHT = (240, 240)\n",
    "X, y = load_data(['../data/JPG_Brain_Scans/yes', '../data/JPG_Brain_Scans/no'], (IMG_WIDTH, IMG_HEIGHT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add channel dimension to X to allow for model training\n",
    "\n",
    "X = X.reshape(-1, 240, 240, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets, retain 20% of data for testing\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate ImageDataGenerator to perform rotations/flips on training dataset\n",
    "\n",
    "datagen = keras.preprocessing.image.ImageDataGenerator(rotation_range=30,\n",
    "                                                          horizontal_flip=True,\n",
    "                                                          vertical_flip=True,\n",
    "                                                          validation_split=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "        keras.Input(shape=(240, 240, 1)),\n",
    "        keras.layers.Conv2D(32, 3, strides=(1, 1), activation='relu', data_format='channels_last', name='conv0'),\n",
    "        keras.layers.MaxPool2D((2, 2), name='max_pool0'),\n",
    "        keras.layers.BatchNormalization(name='bn0'),\n",
    "        keras.layers.Conv2D(64, 3, strides=(1, 1), activation='relu', data_format='channels_last', name='conv1'),\n",
    "        keras.layers.MaxPool2D((2, 2), name='max_pool1'),\n",
    "        keras.layers.BatchNormalization(name='bn1'),\n",
    "        keras.layers.Conv2D(128, 3, strides=(1, 1), activation='relu', data_format='channels_last', name='conv2'),\n",
    "        keras.layers.MaxPool2D((2, 2), name='max_pool2'),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv0 (Conv2D)               (None, 238, 238, 32)      320       \n",
      "_________________________________________________________________\n",
      "max_pool0 (MaxPooling2D)     (None, 119, 119, 32)      0         \n",
      "_________________________________________________________________\n",
      "bn0 (BatchNormalization)     (None, 119, 119, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 117, 117, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pool1 (MaxPooling2D)     (None, 58, 58, 64)        0         \n",
      "_________________________________________________________________\n",
      "bn1 (BatchNormalization)     (None, 58, 58, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 56, 56, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pool2 (MaxPooling2D)     (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 100352)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 100353    \n",
      "=================================================================\n",
      "Total params: 193,409\n",
      "Trainable params: 193,217\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# View summary\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = opt, metrics = ['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 7s 1s/step - loss: 86.0132 - binary_accuracy: 0.6287 - val_loss: 12.3317 - val_binary_accuracy: 0.5882\n"
     ]
    }
   ],
   "source": [
    "# Save tensorboard callback logs for each training epoch\n",
    "\n",
    "from datetime import datetime\n",
    "logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_path = 'models/best_classifier.h5'\n",
    "\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "# Save best model according to its validation set binary accuracy\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=model_path,\n",
    "    monitor='val_binary_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "# Fit model using ImageDataGenerator on training data, unaltered testing data\n",
    "\n",
    "neural_net = model.fit(datagen.flow(X_train, y_train, batch_size=32),\n",
    "                       epochs=100, shuffle=True,\n",
    "                       validation_data = (X_test, y_test), \n",
    "                       callbacks=[tensorboard_callback, model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5490196078431373"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run 'tta_steps' of predictions on test set data which has been rotated/flipped\n",
    "# by the ImageDataGenerator and append each set of predictions to a list.\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "tta_steps = 10\n",
    "predictions = []\n",
    "\n",
    "for i in range(tta_steps):\n",
    "    preds = (model.predict(datagen.flow(X_test, batch_size=32, shuffle=False)) > 0.5).astype(\"int32\")\n",
    "    predictions.append(preds)\n",
    "\n",
    "# Compare the mode of the predictions for each image against the true label, calculate\n",
    "# accuracy\n",
    "\n",
    "np.mean(np.equal(y_test, stats.mode(predictions)[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
